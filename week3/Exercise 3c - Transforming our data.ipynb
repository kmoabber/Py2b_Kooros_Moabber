{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e6f4f9a",
   "metadata": {},
   "source": [
    "<div style=\"background: black; padding: 10px 250px\"><img src=\"https://www.veldikompetens.se/wp-content/themes/consid/static/icons/VeldiKompetens_Logo_Web_Negative.svg\" title=\"Veldi kompetens\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93bffd",
   "metadata": {},
   "source": [
    "<hr><h1><center>Exercise 3c - Transforming our data</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add1cd83",
   "metadata": {},
   "source": [
    "<h3>Instructions </h3>\n",
    "<p> In this exercise you will learn more about concepts such as normalization and transformation. We will be using these tools to make our data into a more valid format for input later into a machine learning model or perhaps for further analysis. Since whichever way we go almost will lead us down the path of statistics when it comes to analyzing a dataset, having normalized and easy to work with values are often a high priority. Lets get to it!\n",
    "\n",
    "A short discussion will also be presented at the end of the section where I discuss the topic of the almost 50% missing bean types.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32acb796",
   "metadata": {},
   "source": [
    "<h3> 1. Necessary Setup </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b14d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Remeber you need to have the file accessible locally! This is the new csv file we created in 3b.\n",
    "# We dont even need the wierd stuff when loading as we did in 3b because we have fixed it! Wow\n",
    "df = pd.read_csv(\"edited_choco.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d44b12",
   "metadata": {},
   "source": [
    "<h3> 2. Transforming and normalizing values</h3>\n",
    "<h4> 2.1 Normalizing values </h4>\n",
    "<p>Commonly in statistical analysis we make use of numbers in the range 0 to 1, this helps a lot for situations where you use clustering or machine learning or plots because otherwise values that are otherwise huge would overtake the smaller ones (for example REF vs Rating). The approach we use here is called Min-max feature scaling and the formula can be be found for example here https://en.wikipedia.org/wiki/Normalization_(statistics). Note: You can use built-in features to perform the normalization but I thought I would showcase the self-coded example<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728616ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading neccesary values for normalization\n",
    "rating_min = df[\"Rating\"].min()\n",
    "rating_max = df[\"Rating\"].max()\n",
    "\n",
    "def normalize_values(col_value):\n",
    "    normalized_val = np.divide(col_value - rating_min, rating_max - rating_min)\n",
    "    return normalized_val\n",
    "\n",
    "df[\"Rating\"] = df[\"Rating\"].apply(normalize_values)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c1fa9",
   "metadata": {},
   "source": [
    "<h4>2.2 Short discussion on REF and Review Date columns </h4>\n",
    "<p>According to the creators of the dataset found here https://www.kaggle.com/rtatman/chocolate-bar-ratings REF and Review dates can be expained as:\n",
    "\n",
    "REF: A value linked to when the review was entered in the database. Higher = more recent.\n",
    "\n",
    "Review Date: Date of publication of the review.\n",
    "\n",
    "This makes transforming the data quite.. wierd? Does it make sense to normalize or standardize these columns and does it even make sense to have them? In my opinion (There is no absolute wrong or right) I would not include the REF columns if I were to train a machine learning algorithm on the data to predict say the rating. This is because REF refers to how recently something was updated, which does in no way at all prove whether or not it has led to a good rating. A ML algorithm does not understand this reasoning but for us as people it makes no sense. Sort of the same can be said about the Review Date but this time a bit more relevant as opinions on a particular taste might vary with trends. I would nevertheless remove that column as well. In short I would create a separate dataset containing the REF and Review dates.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e957f0",
   "metadata": {},
   "source": [
    "<h4>2.3 Converting Cocoa Percent into float </h4>\n",
    "<p>A bit magical in approach; a short explanation: </p>\n",
    "\n",
    "<ul>\n",
    "    <li>.str ensures that we are working on a string object</li>\n",
    "    <li>.rstrip removes a specified character</li>\n",
    "    <li>astype converts it into specified type</li>\n",
    "</ul>\n",
    "\n",
    "Worth noting is that if you run the cell below more than once you will get an error because in that situation the column has already been converted into a float, thus the str.rstrip functionality will yield an error and the code wont work. To circumvent this either only run the cell once or write some code of your own to avoid this! For example you can write an if statement that checks the current type of the column and if its a string do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoa_without_percent = df[\"Cocoa Percent\"].str.rstrip(\"%\")\n",
    "cocoa_in_decimal = cocoa_without_percent.astype(float)/100\n",
    "df[\"Cocoa Percent\"] = cocoa_in_decimal\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c6146",
   "metadata": {},
   "source": [
    "<h4>2.4 Discussion on the Bean Type column </h4> \n",
    "<p>Although there is no coding here I thought I would present my thoughts on the Bean Type column as it might be of interest to some of you. As a reminder, Bean Type was missing for nearly 50% of the samples, thereafter it was broad bean origin with only about 4% of data missing. \n",
    "\n",
    "In some machine learning applications or in statistical analysis even no value can represent a \"class\" or an output. Maybe the algorithm gets trained in a specific way with regards to missing values, i.e in case there is missing values we might score it as this or this rating. When it comes to the Broad Bean Origin which was missing about 4% it is absolutely fine and we can be satisfied with leaving it as is. You can create a fictional label by comparing the broad bean origin to similar samples, but it can also warp the data and is a bit risky. For this situation it is recommended to leave it as is.\n",
    "    \n",
    "Bean Type however is more tricky. With about 50% data missing it will warp statistical and machine learning applications and is definitely considered as the threshold for when to drop or remove an attribute. For most intents and purposes the Bean Type column would make our analysis more inaccurate than precise and it would be wise to remove the attribute. However this will not be done here but if you in the future come back to this dataset and want to train a ML algorithm it is interesting to compare the results with or without the column\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890cc04",
   "metadata": {},
   "source": [
    "<h4>2.5 Open-ended exercise </h4>\n",
    "<p>Here are some suggestions for what you can do yourself to further your knowledge on the area: </p>\n",
    "<ul>\n",
    "    <li>Go to Kaggle.com/datasets and search for a dataset of  your choosing (I recommend staying around less than 1MB in file size). Try to find something with few columns </li>\n",
    "    <li>From scratch yourself perform the follow operations: Identify if there exists empty rows and remove them, drop columns lacking a lot of data, transform data and get a feel for how the data is split with functionalities that has been showcased here in the exercises such as a table to get a general overview of the data represented. </li>\n",
    "</ul>\n",
    "<p>Another suggestion is to keep toying around with this dataset but try creating new entries! Go into the CSV file, add a row with some type of wierd data (say the wrong type, an extreme outlier or something equivalent, then identify how you could identify that type of data in code. An example would be to add a review date entry that is something like 100000, you could then toy around with something like df.boxplot() which is a pandas boxplot good for getting an overview of the data </p>\n",
    "\n",
    "\n",
    "<h4>3. Closing Remarks </h4>\n",
    "<p>Well done! You have now learnt a LOT of the basics of working with data. As I advised in the lecture I highly recommend checking out the youtube channel Computerphile and their series on Data (link can be found in the lecture). Although it is in R it is very informative and I consider it to be one of the best series to start out with when beginning your journey on data analysis </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477ac85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
